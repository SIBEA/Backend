{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chronomanteca/PythonEnv/sibea-backend/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-21 13:35:25.215258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 13:35:26.393552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2023-02-21 13:35:26.393646: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2023-02-21 13:35:26.393655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "import sklearn.metrics.pairwise\n",
    "from simpleneighbors import SimpleNeighbors\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import tensorflow.compat.v2 as tf\n",
    "import torch\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import math\n",
    "import faiss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargue de modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#model_minilm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "path = './models/model_minilm'\n",
    "#model_minilm.save(path)\n",
    "model_minilm = SentenceTransformer(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpnet-base-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#model_mpnet = SentenceTransformer('all-mpnet-base-v2')\n",
    "path = './models/model_mpnet'\n",
    "#model_mpnet.save(path)\n",
    "model_mpnet = SentenceTransformer(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distiluse-base-multilingual-cased-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#model_multilingual_distiluse_v1 = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "path = './models/model_multilingual_distiluse_v1'\n",
    "#model_multilingual_distiluse_v1.save(path)\n",
    "model_multilingual_distiluse_v1 = SentenceTransformer(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distiluse-base-multilingual-cased-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#model_multilingual_distiluse_v2 = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "path = './models/model_multilingual_distiluse_v2'\n",
    "#model_multilingual_distiluse_v2.save(path)\n",
    "model_multilingual_distiluse_v2 = SentenceTransformer(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#model_multilingual_minilm = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "path = './models/model_multilingual_minilm'\n",
    "#model_multilingual_minilm.save(path)\n",
    "model_multilingual_minilm = SentenceTransformer(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paraphrase-multilingual-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#model_multilingual_mpnet = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "path = './models/model_multilingual_mpnet'\n",
    "#model_multilingual_mpnet.save(path)\n",
    "model_multilingual_mpnet = SentenceTransformer(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-encoder-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#model_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "path = './models/universal-sentence-encoder_4'\n",
    "model_use = hub.KerasLayer(path)\n",
    "embeddings = model_use([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
    "print(embeddings.shape, embeddings.dtype)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-enconder-large-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "#model_use_large = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "path = './models/universal-sentence-encoder-large_5'\n",
    "model_use_large = hub.KerasLayer(path)\n",
    "embeddings = model_use_large([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
    "print(embeddings.shape, embeddings.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-encoder-multilingual-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "#model_use_multilingual = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "path = './models/universal-sentence-encoder-multilingual_3'\n",
    "model_use_multilingual = hub.KerasLayer(path)\n",
    "embeddings = model_use_multilingual([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
    "print(embeddings.shape, embeddings.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-encoder-multilingual-large-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "#model_use_multilingual_large = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "path = './models/universal-sentence-encoder-multilingual-large_3'\n",
    "model_use_multilingual_large = hub.KerasLayer(path)\n",
    "embeddings = model_use_multilingual_large([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
    "print(embeddings.shape, embeddings.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación e indexado de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(messages):\n",
    "    result = []\n",
    "    \"\"\"\n",
    "    start_encoding = time.time()\n",
    "    embeddings_minilm = model_minilm.encode(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_minilm.shape[1])\n",
    "    index.add(embeddings_minilm)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_minilm')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"minilm\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_minilm})\n",
    "\n",
    "    start_encoding = time.time()\n",
    "    embeddings_mpnet = model_mpnet.encode(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_mpnet.shape[1])\n",
    "    index.add(embeddings_mpnet)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_mpnet')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"mpnet\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_mpnet})\n",
    "\n",
    "    start_encoding = time.time()\n",
    "    embeddings_multilingual_distiluse_v1 = model_multilingual_distiluse_v1.encode(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_multilingual_distiluse_v1.shape[1])\n",
    "    print(index)\n",
    "    index.add(embeddings_multilingual_distiluse_v1)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_multilingual_distiluse_v1')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"multilingual_distiluse_v1\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_multilingual_distiluse_v1})\n",
    "    \"\"\"\n",
    "    start_encoding = time.time()\n",
    "    embeddings_multilingual_distiluse_v2 = model_multilingual_distiluse_v2.encode(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_multilingual_distiluse_v2.shape[1])\n",
    "    index.add(embeddings_multilingual_distiluse_v2)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_multilingual_distiluse_v2')\n",
    "    end_indexing = time.time()\n",
    "    #print(embeddings_multilingual_distiluse_v2.shape())\n",
    "    result.append({\"model\": \"multilingual_distiluse_v2\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_multilingual_distiluse_v2})\n",
    "    \"\"\"\n",
    "    start_encoding = time.time()\n",
    "    embeddings_multilingual_minilm = model_multilingual_minilm.encode(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_multilingual_minilm.shape[1])\n",
    "    index.add(embeddings_multilingual_minilm)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_multilingual_minilm')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"multilingual_minilm\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_multilingual_minilm})\n",
    "\n",
    "    start_encoding = time.time()\n",
    "    embeddings_multilingual_mpnet = model_multilingual_mpnet.encode(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_multilingual_mpnet.shape[1])\n",
    "    index.add(embeddings_multilingual_mpnet)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_multilingual_mpnet')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"multilingual_mpnet\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_multilingual_mpnet})\n",
    "    \n",
    "    start_encoding = time.time()\n",
    "    embeddings_use = model_use(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_use.shape[1])\n",
    "    index.add(embeddings_use)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_use')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"use\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_use})\n",
    "\n",
    "    start_encoding = time.time()\n",
    "    embeddings_use_large = model_use_large(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_use_large.shape[1])\n",
    "    index.add(embeddings_use_large)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_use_large')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"use_large\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_use_large})\n",
    "\n",
    "    start_encoding = time.time()\n",
    "    embeddings_use_multilingual = model_use_multilingual(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_use_multilingual.shape[1])\n",
    "    index.add(embeddings_use_multilingual)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_use_multilingual')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"use_multilingual\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_use_multilingual})\n",
    "\n",
    "    start_encoding = time.time()\n",
    "    embeddings_use_multilingual_large = model_use_multilingual_large(messages)\n",
    "    end_encoding = time.time()\n",
    "    start_indexing = time.time()\n",
    "    index = faiss.IndexFlatL2(embeddings_use_multilingual_large.shape[1])\n",
    "    index.add(embeddings_use_multilingual_large)\n",
    "    faiss.write_index(index, './indexes/index_embeddings_use_multilingual_large')\n",
    "    end_indexing = time.time()\n",
    "    result.append({\"model\": \"use_multilingual_large\", \"time_encoding\": end_encoding - start_encoding, \"time_indexing\": end_indexing - start_indexing, \"tensor\": embeddings_use_multilingual_large})\n",
    "    \"\"\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapa de calor de similaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation, ax, title):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\",\n",
    "      ax=ax\n",
    "      )\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapa de calor todos los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_embeddings_heatmap(embeddings, messages):\n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(ncols=2, nrows=5, sharey=True, figsize=(30, 70), constrained_layout=True)\n",
    "    for (embedding, ax) in zip(embeddings, [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10]):\n",
    "        plot_similarity(messages, embedding[\"tensor\"], 90, ax, embedding[\"model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapa de calor tiempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_embeddings_timing(embeddings, measure=\"time_encoding\"):\n",
    "    fig, ax = plt.subplots(figsize=(30, 10))\n",
    "    times = [embedding[measure] for embedding in embeddings]\n",
    "    ax.bar([embedding[\"model\"] for embedding in embeddings], times)\n",
    "    if (measure == \"time_encoding\"):\n",
    "        ax.set_title(\"Time to generate embeddings\")\n",
    "    else:\n",
    "        ax.set_title(\"Time to index embeddings\")\n",
    "    ax.set_ylabel(\"Time in seconds\")\n",
    "    ax.set_xlabel(\"Model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plot_embeddings_heatmap(embeddings, messages)\n",
    "plot_embeddings_timing(embeddings, \"time_encoding\")\n",
    "plot_embeddings_timing(embeddings, \"time_indexing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(17, 512)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    \"I like my phone\",\n",
    "    \"My cellphone is not good.\",\n",
    "    \"I'm going to call someone with my phone\",\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "    \"Healthcare system\",\n",
    "    \"COVID-19\",\n",
    "    \"ADHD\",\n",
    "    \"CPAP\",\n",
    "    \"Lungs\",\n",
    "    \"Psychology\",\n",
    "    \"BMI\",\n",
    "    \"Fat\"\n",
    "]\n",
    "embeddings = generate_embeddings(messages)\n",
    "print(type(embeddings[0].get(\"tensor\")))\n",
    "tensor = embeddings[0].get(\"tensor\")\n",
    "print(tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "term = \"lluvia\"\n",
    "query = model_multilingual_distiluse_v2.encode(term)\n",
    "file = open(\"./queries/test.txt\", \"w\")\n",
    "file.write(str(query))\n",
    "file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generacion de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def search(model, corpus, query, top_k=5, search_method=\"sbert\", model_source=\"sbert\"):\n",
    "    result = {}\n",
    "    \n",
    "    start = time.time()\n",
    "    if model_source == \"sbert\":\n",
    "        query_embeddings = model.encode([query])\n",
    "        corpus_embeddings = corpus[\"tensor\"]\n",
    "    elif model_source == \"tf\":\n",
    "        query_embeddings = model([query]).numpy()\n",
    "        corpus_embeddings = corpus[\"tensor\"].numpy()\n",
    "    \n",
    "    \n",
    "    if search_method == \"sbert\":\n",
    "        hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k)\n",
    "        top_results = np.array([hit['corpus_id'] for hit in hits[0]])\n",
    "    elif search_method == \"torch\":\n",
    "        cos_scores = util.cos_sim(query_embeddings, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k).indices.tolist()\n",
    "    elif search_method == \"numpy\":\n",
    "        cos_scores = util.cos_sim(query_embeddings, corpus_embeddings)[0]\n",
    "        top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "    elif search_method == \"faiss\":\n",
    "        index = faiss.read_index(\"./indexes/index_embeddings_\" + corpus[\"model\"])\n",
    "        top_results = index.search(query_embeddings, top_k)[1][0]\n",
    "    elif search_method == \"tf\":\n",
    "        cos_scores = tf.keras.losses.cosine_similarity(query_embeddings, corpus_embeddings, axis=1)\n",
    "        top_results = tf.math.top_k(cos_scores, k=top_k)\n",
    "        top_results = (top_results.indices.numpy(), top_results.values.numpy())[0]\n",
    "    \n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    result = {\n",
    "        \"results\": top_results,\n",
    "        \"time\": end - start,\n",
    "        \"model\": corpus[\"model\"],\n",
    "        \"search_method\": search_method,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busqueda con todos los métodos de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def search_all(query, k, embeddings):\n",
    "    total = []\n",
    "    \n",
    "    results = []\n",
    "    results.append(search(model_multilingual_distiluse_v1, embeddings[2], query, k, \"sbert\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v1, embeddings[2], query, k, \"torch\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v1, embeddings[2], query, k, \"numpy\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v1, embeddings[2], query, k, \"faiss\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v1, embeddings[2], query, k, \"tf\", model_source=\"sbert\"))\n",
    "    total.append(results)\n",
    "\n",
    "    results = []\n",
    "    results.append(search(model_multilingual_distiluse_v2, embeddings[3], query, k, \"sbert\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v2, embeddings[3], query, k, \"torch\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v2, embeddings[3], query, k, \"numpy\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v2, embeddings[3], query, k, \"faiss\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_distiluse_v2, embeddings[3], query, k, \"tf\", model_source=\"sbert\"))\n",
    "    total.append(results)\n",
    "    \n",
    "    results = []\n",
    "    results.append(search(model_multilingual_minilm, embeddings[4], query, k, \"sbert\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_minilm, embeddings[4], query, k, \"torch\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_minilm, embeddings[4], query, k, \"numpy\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_minilm, embeddings[4], query, k, \"faiss\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_minilm, embeddings[4], query, k, \"tf\", model_source=\"sbert\"))\n",
    "    total.append(results)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    results.append(search(model_multilingual_mpnet, embeddings[5], query, k, \"sbert\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_mpnet, embeddings[5], query, k, \"torch\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_mpnet, embeddings[5], query, k, \"numpy\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_mpnet, embeddings[5], query, k, \"faiss\", model_source=\"sbert\"))\n",
    "    results.append(search(model_multilingual_mpnet, embeddings[5], query, k, \"tf\", model_source=\"sbert\"))\n",
    "    total.append(results)\n",
    "   \n",
    "    results = []\n",
    "    results.append(search(model_use_multilingual, embeddings[8], query, k, \"sbert\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual, embeddings[8], query, k, \"torch\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual, embeddings[8], query, k, \"numpy\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual, embeddings[8], query, k, \"faiss\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual, embeddings[8], query, k, \"tf\", model_source=\"tf\"))\n",
    "    total.append(results)\n",
    "\n",
    "    results = []\n",
    "    results.append(search(model_use_multilingual_large, embeddings[9], query, k, \"sbert\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual_large, embeddings[9], query, k, \"torch\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual_large, embeddings[9], query, k, \"numpy\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual_large, embeddings[9], query, k, \"faiss\", model_source=\"tf\"))\n",
    "    results.append(search(model_use_multilingual_large, embeddings[9], query, k, \"tf\", model_source=\"tf\"))\n",
    "    total.append(results)\n",
    "    \"\"\"\n",
    "\n",
    "    return total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapa de calor de tiempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_search_timing(results):\n",
    "    times = [[result[\"time\"] for result in results] for results in results]\n",
    "    plot = sns.heatmap(times, annot=True, fmt=\".3f\", xticklabels=[\"sbert\", \"torch\", \"numpy\", \"faiss\", \"tf\"], yticklabels=[\"multilingual_distiluse_v1\", \"multilingual_distiluse_v2\", \"multilingual_minilm\", \"multilingual_mpnet\", \"use_multilingual\", \"use_multilingual_large\"])\n",
    "    plot.set_title(\"Time to search\")\n",
    "    plot.set_ylabel(\"Model\")\n",
    "    plot.set_xlabel(\"Search method\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def search_results(results, messages, model_index):\n",
    "    for index, result in enumerate(results):\n",
    "        print(\"Model #\" + str(index) + \" - \" + result[0]['model'])\n",
    "        for i, indexes in enumerate(result):\n",
    "            if (index == model_index):\n",
    "                print(indexes[\"search_method\"] + \" - \", end=\"\")\n",
    "                print(str([messages[i] for i in indexes[\"results\"]]) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "results = search_all(\"Pobreza\", 5, embeddings)\n",
    "#plot_search_timing(results)\n",
    "search_results(results, messages, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titulos investigaciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacion y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_proyecto', 'id_propuesta', 'tipo_de_propuesta',\n",
      "       'fecha_de_negociacion', 'ano', 'titulo_del_proyecto',\n",
      "       'fecha_inicial_real', 'fecha_final_real', 'nombre_facultad',\n",
      "       'nombre_del_departamento', 'convocatoria', 'nombre_patrocinador',\n",
      "       'nombre_grupo_de_investigacion', 'miembro_del_equipo',\n",
      "       'tipo_de_documento', 'numero_de_documento', 'nombres_y_apellidos',\n",
      "       'rol_en_el_proyecto', 'porcentaje_dedicacion', 'codigo_presupuesto',\n",
      "       'codigo_contrato_secre_juridica', 'valor_aprobado_patrocinador',\n",
      "       'contrapartida_terceros', 'contrapartida_rec_propi',\n",
      "       'contrapartida_rec_nuevo', 'rec_nuevos_unidade_academica',\n",
      "       'valor_contrapartida_total', 'valor_total', 'estado', 'codigo_tipo',\n",
      "       'convocatoria_nombre_corto', 'convocatoria', 'tipo_de_actividad',\n",
      "       'fecha_estimada_inicio', 'fecha_estimada_fin',\n",
      "       'id_empleado_responsable_puj', 'nombre_responsable_puj',\n",
      "       'departamento_responsable_puj', 'id_departamento_responsable_puj',\n",
      "       'facultad_responsable_puj', 'nombre_financiador_principal', 'resumen',\n",
      "       'objetivos', 'metodologia', 'categoria', 'producto',\n",
      "       'valor_solicitado_a_financiador_principal', 'valor_total_financiadores',\n",
      "       'tipo_de_investigacion', 'gran_area', 'objetivo_socioeconomico',\n",
      "       'palabras_clave', 'aprobado?', 'descripcion'],\n",
      "      dtype='object')\n",
      "(2853, 54)\n"
     ]
    }
   ],
   "source": [
    "inv_df = pd.read_excel('./xlsx/InvestigarPUJ.xlsx', sheet_name='Hoja1', converters={'ID PROYECTO':str})\n",
    "siap_df = pd.read_excel('./xlsx/Descriptores SIAP 2023.xlsx', sheet_name='SIAP ', converters={'ID Proy':str})\n",
    "inv_full_df = pd.read_excel('./xlsx/Descriptores SIAP 2023.xlsx', sheet_name='InvestigarPUJ', converters={'Id':str})\n",
    "\n",
    "inv_full_df.drop(\"Año\", axis=1, inplace=True)\n",
    "\n",
    "datasets = [inv_df, siap_df, inv_full_df]\n",
    "for dataset in datasets:\n",
    "    dataset.replace('\\\\N', np.NaN, inplace=True)\n",
    "    dataset.replace('null', np.NaN, inplace=True)\n",
    "    dataset.replace('nan', np.NaN, inplace=True)\n",
    "    dataset.replace('N/A', np.NaN, inplace=True)\n",
    "\n",
    "\n",
    "merged = inv_df.merge(siap_df, left_on=\"ID PROYECTO\", right_on=\"ID Proy\", how=\"left\")\n",
    "df = merged.merge(inv_full_df, left_on=\"ID PROYECTO\", right_on=\"Id\", how=\"left\")\n",
    "\n",
    "df = df.groupby(\"ID PROYECTO\").agg(lambda x: list(set(x))).applymap(lambda x: ', '.join(str(y) for y in x if str(y) != 'nan') if isinstance(x, list) else x)\n",
    "df = df.reset_index()\n",
    "df.columns = [unidecode(x.lower().strip().replace(\" \", \"_\").replace(\"__\", \"_\").replace(\".\", \"\")) for x in df.columns]\n",
    "\n",
    "df.replace('', np.NaN, inplace=True)\n",
    "\n",
    "df[\"descripcion\"] = df[\"descripcion_x\"].fillna(\"\") + df[\"descripcion_y\"].fillna(\"\")\n",
    "\n",
    "df.drop(\"titulo_x\", axis=1, inplace=True)\n",
    "df.drop(\"titulo_y\", axis=1, inplace=True)\n",
    "df.drop(\"id_proy\", axis=1, inplace=True)\n",
    "df.drop(\"id\", axis=1, inplace=True)\n",
    "df.drop(\"financiador\", axis=1, inplace=True)\n",
    "df.drop(\"tipo_propuesta\", axis=1, inplace=True)\n",
    "df.drop(\"descripcion_x\", axis=1, inplace=True)\n",
    "df.drop(\"descripcion_y\", axis=1, inplace=True)\n",
    "df.drop(\"id_propt\", axis=1, inplace=True)\n",
    "df.drop(\"f_inic_real\", axis=1, inplace=True)\n",
    "df.drop(\"f_final_real\", axis=1, inplace=True)\n",
    "df.drop(\"facultad\", axis=1, inplace=True)\n",
    "df.drop(\"departamento\", axis=1, inplace=True)\n",
    "df.drop(\"estado_proyecto\", axis=1, inplace=True)\n",
    "df.drop(\"nombre\", axis=1, inplace=True)\n",
    "df.drop(\"cantidad\", axis=1, inplace=True)\n",
    "\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "#df[df[\"id_proyecto\"] == \"000000000007161\"]\n",
    "#df[df[\"id_proyecto\"] == \"004438\"]\n",
    "#df[df[\"id_proyecto\"] == \"20104\"]\n",
    "\n",
    "df[\"corpus\"] = df[\"titulo_del_proyecto\"].fillna(\"\") + \" \"  + \\\n",
    "df[\"nombre_facultad\"].str.split().str[-1].fillna(\"\") + \" \" + \\\n",
    "df[\"nombre_del_departamento\"].str.split().str[-1].fillna(\"\") + \" \" + \\\n",
    "df[\"descripcion\"].fillna(\"\") + \" \" + \\\n",
    "df[\"resumen\"].fillna(\"\") + \" \" + \\\n",
    "df[\"objetivos\"].fillna(\"\") + \" \" + \\\n",
    "df[\"metodologia\"].fillna(\"\") + \" \" + \\\n",
    "df[\"gran_area\"].fillna(\"\") + \" \" + \\\n",
    "df[\"objetivo_socioeconomico\"].fillna(\"\") + \" \" + \\\n",
    "df[\"palabras_clave\"].fillna(\"\")\n",
    "\n",
    "df[\"corpus\"] = df[\"corpus\"].replace(r'\\n',' ', regex=True).str.strip()\n",
    "\n",
    "messages = df[\"corpus\"].head(1000).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df[\"corpus\"].to_csv(\"./corpus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_proyecto</th>\n",
       "      <th>id_propuesta</th>\n",
       "      <th>tipo_de_propuesta</th>\n",
       "      <th>fecha_de_negociacion</th>\n",
       "      <th>ano</th>\n",
       "      <th>titulo_del_proyecto</th>\n",
       "      <th>fecha_inicial_real</th>\n",
       "      <th>fecha_final_real</th>\n",
       "      <th>nombre_facultad</th>\n",
       "      <th>nombre_del_departamento</th>\n",
       "      <th>convocatoria</th>\n",
       "      <th>nombre_patrocinador</th>\n",
       "      <th>nombre_grupo_de_investigacion</th>\n",
       "      <th>miembro_del_equipo</th>\n",
       "      <th>tipo_de_documento</th>\n",
       "      <th>numero_de_documento</th>\n",
       "      <th>nombres_y_apellidos</th>\n",
       "      <th>rol_en_el_proyecto</th>\n",
       "      <th>porcentaje_dedicacion</th>\n",
       "      <th>codigo_presupuesto</th>\n",
       "      <th>codigo_contrato_secre_juridica</th>\n",
       "      <th>valor_aprobado_patrocinador</th>\n",
       "      <th>contrapartida_terceros</th>\n",
       "      <th>contrapartida_rec_propi</th>\n",
       "      <th>contrapartida_rec_nuevo</th>\n",
       "      <th>rec_nuevos_unidade_academica</th>\n",
       "      <th>valor_contrapartida_total</th>\n",
       "      <th>valor_total</th>\n",
       "      <th>estado</th>\n",
       "      <th>codigo_tipo</th>\n",
       "      <th>convocatoria_nombre_corto</th>\n",
       "      <th>convocatoria</th>\n",
       "      <th>tipo_de_actividad</th>\n",
       "      <th>fecha_estimada_inicio</th>\n",
       "      <th>fecha_estimada_fin</th>\n",
       "      <th>id_empleado_responsable_puj</th>\n",
       "      <th>nombre_responsable_puj</th>\n",
       "      <th>departamento_responsable_puj</th>\n",
       "      <th>id_departamento_responsable_puj</th>\n",
       "      <th>facultad_responsable_puj</th>\n",
       "      <th>nombre_financiador_principal</th>\n",
       "      <th>resumen</th>\n",
       "      <th>objetivos</th>\n",
       "      <th>metodologia</th>\n",
       "      <th>categoria</th>\n",
       "      <th>producto</th>\n",
       "      <th>valor_solicitado_a_financiador_principal</th>\n",
       "      <th>valor_total_financiadores</th>\n",
       "      <th>tipo_de_investigacion</th>\n",
       "      <th>gran_area</th>\n",
       "      <th>objetivo_socioeconomico</th>\n",
       "      <th>palabras_clave</th>\n",
       "      <th>aprobado?</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>000000000007161</td>\n",
       "      <td>00007029</td>\n",
       "      <td>PUE</td>\n",
       "      <td>2015-07-27 00:00:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>Constitución del Centro de Excelencia y Apropi...</td>\n",
       "      <td>2015-11-06 00:00:00</td>\n",
       "      <td>2019-05-31 00:00:00</td>\n",
       "      <td>Facultad de Ingeniería</td>\n",
       "      <td>Dpto. de Electrónica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEPARTAMENTO ADMINISTRATIVO DE CIENCIA,</td>\n",
       "      <td>CEPIT: Sistemas de Control, Electrónica de Pot...</td>\n",
       "      <td>00010153314, 00010123188, 0031139, 00010032943...</td>\n",
       "      <td>SE, CC</td>\n",
       "      <td>80095329, 1010164261, 16865144, 16771142, 1015...</td>\n",
       "      <td>Mendez Chaves,Diego, Cotrino Badillo,Carlos, C...</td>\n",
       "      <td>COIN, PI</td>\n",
       "      <td>0.0, 10.0, 100.0, 50.0</td>\n",
       "      <td>121300C0101103</td>\n",
       "      <td>Contrato fp44842-502-2015</td>\n",
       "      <td>6354717309.0</td>\n",
       "      <td>2164600000.0</td>\n",
       "      <td>10400000.0</td>\n",
       "      <td>70000000.0</td>\n",
       "      <td>200000000</td>\n",
       "      <td>0</td>\n",
       "      <td>8799717309.0</td>\n",
       "      <td>Pendiente Compromiso</td>\n",
       "      <td>OBJGE, OBJES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diseñar, montar e iniciar la operación de un C...</td>\n",
       "      <td>Constitución del Centro de Excelencia y Apropi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id_proyecto id_propuesta tipo_de_propuesta fecha_de_negociacion  \\\n",
       "710  000000000007161     00007029               PUE  2015-07-27 00:00:00   \n",
       "\n",
       "      ano                                titulo_del_proyecto  \\\n",
       "710  2015  Constitución del Centro de Excelencia y Apropi...   \n",
       "\n",
       "      fecha_inicial_real     fecha_final_real         nombre_facultad  \\\n",
       "710  2015-11-06 00:00:00  2019-05-31 00:00:00  Facultad de Ingeniería   \n",
       "\n",
       "    nombre_del_departamento convocatoria  \\\n",
       "710    Dpto. de Electrónica          NaN   \n",
       "\n",
       "                         nombre_patrocinador  \\\n",
       "710  DEPARTAMENTO ADMINISTRATIVO DE CIENCIA,   \n",
       "\n",
       "                         nombre_grupo_de_investigacion  \\\n",
       "710  CEPIT: Sistemas de Control, Electrónica de Pot...   \n",
       "\n",
       "                                    miembro_del_equipo tipo_de_documento  \\\n",
       "710  00010153314, 00010123188, 0031139, 00010032943...            SE, CC   \n",
       "\n",
       "                                   numero_de_documento  \\\n",
       "710  80095329, 1010164261, 16865144, 16771142, 1015...   \n",
       "\n",
       "                                   nombres_y_apellidos rol_en_el_proyecto  \\\n",
       "710  Mendez Chaves,Diego, Cotrino Badillo,Carlos, C...           COIN, PI   \n",
       "\n",
       "      porcentaje_dedicacion codigo_presupuesto codigo_contrato_secre_juridica  \\\n",
       "710  0.0, 10.0, 100.0, 50.0     121300C0101103      Contrato fp44842-502-2015   \n",
       "\n",
       "    valor_aprobado_patrocinador contrapartida_terceros  \\\n",
       "710                6354717309.0           2164600000.0   \n",
       "\n",
       "    contrapartida_rec_propi contrapartida_rec_nuevo  \\\n",
       "710              10400000.0              70000000.0   \n",
       "\n",
       "    rec_nuevos_unidade_academica valor_contrapartida_total   valor_total  \\\n",
       "710                    200000000                         0  8799717309.0   \n",
       "\n",
       "                   estado   codigo_tipo convocatoria_nombre_corto  \\\n",
       "710  Pendiente Compromiso  OBJGE, OBJES                       NaN   \n",
       "\n",
       "    convocatoria tipo_de_actividad fecha_estimada_inicio fecha_estimada_fin  \\\n",
       "710          NaN               NaN                   NaT                NaT   \n",
       "\n",
       "    id_empleado_responsable_puj nombre_responsable_puj  \\\n",
       "710                         NaN                    NaN   \n",
       "\n",
       "    departamento_responsable_puj id_departamento_responsable_puj  \\\n",
       "710                          NaN                             NaN   \n",
       "\n",
       "    facultad_responsable_puj nombre_financiador_principal resumen  objetivos  \\\n",
       "710                      NaN                          NaN     NaN        NaN   \n",
       "\n",
       "    metodologia categoria producto valor_solicitado_a_financiador_principal  \\\n",
       "710         NaN       NaN      NaN                                      NaN   \n",
       "\n",
       "    valor_total_financiadores tipo_de_investigacion gran_area  \\\n",
       "710                       NaN                   NaN       NaN   \n",
       "\n",
       "    objetivo_socioeconomico palabras_clave aprobado?  \\\n",
       "710                     NaN            NaN       NaN   \n",
       "\n",
       "                                           descripcion  \\\n",
       "710  Diseñar, montar e iniciar la operación de un C...   \n",
       "\n",
       "                                                corpus  \n",
       "710  Constitución del Centro de Excelencia y Apropi...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"id_proyecto\"] == \"000000000007161\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "categories = df[\"nombre_facultad\"].groupby(df[\"nombre_facultad\"]).count().sort_values(ascending=False)\n",
    "top20 = df[\"titulo_del_proyecto\"].drop_duplicates().str.split(expand=True).stack().value_counts()[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generacion embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando tensores generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "minilm_tensor = embeddings[0].get('tensor')\n",
    "mpnet_tensor = embeddings[1].get('tensor')\n",
    "multilingual_distiluse_v1_tensor = embeddings[2].get('tensor')\n",
    "multilingual_distiluse_v2_tensor = embeddings[3].get('tensor')\n",
    "multilingual_minilm_tensor = embeddings[4].get('tensor')\n",
    "\n",
    "np.save('./tensors/binary/minilm_tensor.npy', minilm_tensor)\n",
    "np.save('./tensors/binary/mpnet_tensor.npy', mpnet_tensor)\n",
    "np.save('./tensors/binary/multilingual_distiluse_v1_tensor.npy', multilingual_distiluse_v1_tensor)\n",
    "np.save('./tensors/binary/multilingual_distiluse_v2_tensor.npy', multilingual_distiluse_v2_tensor)\n",
    "np.save('./tensors/binary/multilingual_minilm_tensor.npy', multilingual_minilm_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando tensores generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "embeddings.append({\"model\":\"minilm\",\"tensor\":np.load('./tensor/binary/minilm_tensor.npy')})\n",
    "embeddings.append({\"model\":\"mpnet\",\"tensor\":np.load('./tensor/binary/mpnet_tensor.npy')})\n",
    "embeddings.append({\"model\":\"multilingual_distiluse_v1\",\"tensor\":np.load('./tensor/binary/multilingual_distiluse_v1_tensor.npy')})\n",
    "embeddings.append({\"model\":\"multilingual_distiluse_v2\",\"tensor\":np.load('./tensor/binary/multilingual_distiluse_v2_tensor.npy')})\n",
    "embeddings.append({\"model\":\"multilingual_minilm\",\"tensor\":np.load('./tensor/binary/multilingual_minilm_tensor.npy')})\n",
    "print(embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando como CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('./tensors/csv/minilm_tensor.csv', minilm_tensor,delimiter=\",\")\n",
    "np.savetxt('./tensors/csv/mpnet_tensor.csv', mpnet_tensor,delimiter=\",\")\n",
    "np.savetxt('./tensors/csv/multilingual_distiluse_v1_tensor.csv', multilingual_distiluse_v1_tensor,delimiter=\",\")\n",
    "np.savetxt('./tensors/csv/multilingual_distiluse_v2_tensor.csv', multilingual_distiluse_v2_tensor,delimiter=\",\")\n",
    "np.savetxt('./tensors/csv/multilingual_minilm_tensor.csv', multilingual_minilm_tensor,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 384)\n",
      "(1000, 768)\n",
      "(1000, 512)\n",
      "(1000, 512)\n",
      "(1000, 384)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m arr \u001b[39m=\u001b[39m embeddings[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(arr\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m arr \u001b[39m=\u001b[39m embeddings[\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(arr\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "arr = embeddings[0].get(\"tensor\")\n",
    "print(arr.shape)\n",
    "arr = embeddings[1].get(\"tensor\")\n",
    "print(arr.shape)\n",
    "arr = embeddings[2].get(\"tensor\")\n",
    "print(arr.shape)\n",
    "arr = embeddings[3].get(\"tensor\")\n",
    "print(arr.shape)\n",
    "arr = embeddings[4].get(\"tensor\")\n",
    "print(arr.shape)\n",
    "arr = embeddings[5].get(\"tensor\")\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plot_embeddings_timing(embeddings, \"time_encoding\")\n",
    "plot_embeddings_timing(embeddings, \"time_indexing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento de guardar los embeddings en disco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the number of columns changed from 384 to 768 at row 4001; use `usecols` to select a subset and avoid this error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(\u001b[39m'\u001b[39;49m\u001b[39membeddings.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint(type(embeddings[0]))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint(type(embeddings[0].get('tensor')))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m        np.savetxt(f,embedding.get('tensor'),delimiter=',')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/PythonEnv/sibea-backend/lib/python3.10/site-packages/numpy/lib/npyio.py:1356\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1354\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1356\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[1;32m   1357\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[1;32m   1358\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1359\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[1;32m   1361\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/PythonEnv/sibea-backend/lib/python3.10/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    996\u001b[0m     data \u001b[39m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m    998\u001b[0m \u001b[39mif\u001b[39;00m read_dtype_via_object_chunks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     arr \u001b[39m=\u001b[39m _load_from_filelike(\n\u001b[1;32m   1000\u001b[0m         data, delimiter\u001b[39m=\u001b[39;49mdelimiter, comment\u001b[39m=\u001b[39;49mcomment, quote\u001b[39m=\u001b[39;49mquote,\n\u001b[1;32m   1001\u001b[0m         imaginary_unit\u001b[39m=\u001b[39;49mimaginary_unit,\n\u001b[1;32m   1002\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols, skiplines\u001b[39m=\u001b[39;49mskiplines, max_rows\u001b[39m=\u001b[39;49mmax_rows,\n\u001b[1;32m   1003\u001b[0m         converters\u001b[39m=\u001b[39;49mconverters, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1004\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding, filelike\u001b[39m=\u001b[39;49mfilelike,\n\u001b[1;32m   1005\u001b[0m         byte_converters\u001b[39m=\u001b[39;49mbyte_converters)\n\u001b[1;32m   1007\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[39m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[39m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     \u001b[39mif\u001b[39;00m filelike:\n",
      "\u001b[0;31mValueError\u001b[0m: the number of columns changed from 384 to 768 at row 4001; use `usecols` to select a subset and avoid this error"
     ]
    }
   ],
   "source": [
    "embeddings = np.loadtxt('embeddings.csv', delimiter=',')\n",
    "\"\"\"\n",
    "print(type(embeddings[0]))\n",
    "print(type(embeddings[0].get('tensor')))\n",
    "f = open('embeddings.csv', 'a')\n",
    "for embedding in embeddings:\n",
    "    for key in embedding:\n",
    "        #print(embedding.get('tensor'))\n",
    "        f.write(\"\\n\")\n",
    "        np.savetxt(f,embedding.get('tensor'),delimiter=',')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "results = search_all(\"Explotacion infantil\", 10, embeddings)\n",
    "plot_search_timing(results)\n",
    "search_results(results, messages, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sibea",
   "language": "python",
   "name": "sibea"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
